# -*- coding: utf-8 -*-
"""Data Tacticians.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11gCEPRlMY6JECOqCuPOEWgs_ATa-FLqA
"""

import pandas as pd

# Load Training Data
train_url = "https://coding-platform.s3.amazonaws.com/dev/lms/tickets/4c8465f0-fce0-484f-8497-d25feaa8e995/NqndMEyZakuimmFI.csv"
df_train = pd.read_csv(train_url)
print("Training data shape:", df_train.shape)
print("Training preview:")
display(df_train.head())

# Load Test Data
test_url = "https://coding-platform.s3.amazonaws.com/dev/lms/tickets/cab5b1bf-9132-4399-8ed5-2c049fcc89f8/0tkf3jUGLYjCEJGz.csv"
df_test = pd.read_csv(test_url)
print("\nTest data shape:", df_test.shape)
print("Test preview:")
display(df_test.head())

# Step 2: Clean and Explore the Training Data

# Check for missing values
print("Missing values per column:")
print(df_train.isnull().sum())

# Check target column distribution
print("\nTarget class distribution:")
print(df_train['fraudulent'].value_counts())

# % of fake job listings
fraud_percent = (df_train['fraudulent'].sum() / len(df_train)) * 100
print(f"\nFraudulent Job Listings: {fraud_percent:.2f}%")

# Fill missing values with empty strings to avoid errors later
df_train.fillna('', inplace=True)
df_test.fillna('', inplace=True)

# Preview important columns
df_train[['title', 'company_profile', 'description', 'requirements', 'fraudulent']].head()

from sklearn.feature_extraction.text import TfidfVectorizer
import re

# Step 3: Combine useful text fields into one
def combine_text_fields(row):
    return f"{row['title']} {row['company_profile']} {row['description']} {row['requirements']}"

df_train['text'] = df_train.apply(combine_text_fields, axis=1)
df_test['text'] = df_test.apply(combine_text_fields, axis=1)

# Basic text cleaning function
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)  # remove punctuation
    text = re.sub(r'\d+', '', text)      # remove numbers
    return text

# Apply cleaning
df_train['text'] = df_train['text'].apply(clean_text)
df_test['text'] = df_test['text'].apply(clean_text)

# TF-IDF Vectorization (fit only on training text)
vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')
X_train = vectorizer.fit_transform(df_train['text'])
X_test = vectorizer.transform(df_test['text'])

# Target variable
y_train = df_train['fraudulent']

print("TF-IDF vectorization complete.")
print("X_train shape:", X_train.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report, confusion_matrix

# Split training data into 80% train, 20% validation
X_tr, X_val, y_tr, y_val = train_test_split(
    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42
)

# Train logistic regression with class imbalance handling
model = LogisticRegression(class_weight='balanced', max_iter=1000)
model.fit(X_tr, y_tr)

# Predict on validation set
y_pred = model.predict(X_val)

# Evaluate F1 Score
f1 = f1_score(y_val, y_pred)
print(f"✅ F1 Score: {f1:.4f}")

# Optional: Full classification report
print("\nClassification Report:")
print(classification_report(y_val, y_pred))

import numpy as np
import pandas as pd

# Predict probabilities (second column = probability of class 1 → fraud)
probabilities = model.predict_proba(X_test)[:, 1]

# Final predictions (0 or 1)
predictions = model.predict(X_test)

# Save in dataframe
df_results = df_test.copy()
df_results['fraud_probability'] = probabilities
df_results['fraud_prediction'] = predictions

# Preview top 10 most suspicious job postings
top_suspicious = df_results.sort_values(by='fraud_probability', ascending=False).head(10)
print("Top 10 most suspicious job listings:\n")
display(top_suspicious[['title', 'location', 'fraud_probability']])

# Optional: Save to CSV if needed
# df_results.to_csv("job_fraud_predictions.csv", index=False)

!pip install plotly --quiet
import plotly.express as px

# Pie Chart: Real vs Fake predictions
pie_chart = px.pie(df_results, names='fraud_prediction',
                   title='Genuine vs Fraudulent Job Predictions',
                   color_discrete_sequence=px.colors.qualitative.Set2,
                   labels={0: 'Genuine', 1: 'Fraudulent'})
pie_chart.show()

# Histogram: Fraud Probability Distribution
histogram = px.histogram(df_results, x='fraud_probability',
                         nbins=50, title='Fraud Probability Distribution',
                         color_discrete_sequence=['#EF553B'])
histogram.show()

# Table: Top 10 Suspicious Jobs
top10 = df_results.sort_values(by='fraud_probability', ascending=False).head(10)
top10_table = top10[['title', 'location', 'fraud_probability']]
print("Top 10 Most Suspicious Jobs:")
display(top10_table)

pip install streamlit plotly pandas scikit-learn

import joblib

# Save your trained model and vectorizer
joblib.dump(model, "model.pkl")
joblib.dump(vectorizer, "vectorizer.pkl")

# Download to your PC
from google.colab import files
files.download("model.pkl")
files.download("vectorizer.pkl")

import joblib
joblib.dump(model, 'model.joblib')

from google.colab import files
files.download('model.joblib')

import joblib

# After training
joblib.dump(vectorizer, 'vectorizer.joblib')

from google.colab import files
files.download('vectorizer.joblib')

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import joblib

# 1. Load your dataset
df = pd.read_csv('your_dataset.csv')  # Replace with your actual file
df.dropna(subset=['job_description', 'label'], inplace=True)  # ensure clean data

# 2. Split features and labels
X = df['job_description']
y = df['label']  # 0 for genuine, 1 for scam (adjust based on your data)

# 3. Vectorization
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X_vec = vectorizer.fit_transform(X)

# 4. Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)

# 5. Model training
model = LogisticRegression()
model.fit(X_train, y_train)

# 6. Save the model and vectorizer
joblib.dump(model, 'model.joblib')
joblib.dump(vectorizer, 'vectorizer.joblib')

print("✅ Model and vectorizer saved successfully!")

# Step 1: Upload the dataset
from google.colab import files
uploaded = files.upload()